# ğŸŒ LinguaLift: Your Smart Multimodal Travel Assistant

**LinguaLift** is an AI-powered travel assistant that helps users translate text, speech, and images in real-time, while also suggesting tourist spots using Azure OpenAI. Designed with empathy and accessibility in mind, it bridges the communication gap for travelers â€” enabling smoother, smarter, and more confident exploration.

---

## ğŸš€ Features

- ğŸ” **Real-time Translation** â€“ Text, speech, and image translation across multiple languages
- ğŸ™ï¸ **Voice Input** â€“ Speak and get instant translation
- ğŸ–¼ï¸ **Image Text Recognition** â€“ Extract and translate text from menus, signs, and documents
- ğŸ¤– **AI-Powered Tourist Suggestions** â€“ Get relevant tourist spot recommendations using Azure OpenAI
- ğŸŒ“ **Dark Mode** â€“ Toggle between light and dark themes
- ğŸ“‹ **Copy-to-Clipboard** â€“ Easily copy translated content
- ğŸŒ **Multi-language Support** â€“ Translate between English, Spanish, French, Bengali, Hindi, and more

---

## ğŸ’¡ Inspiration

Travelers often face communication barriers in unfamiliar countries. We wanted to create a solution that not only translates but empowers â€” turning confusion into confidence, and isolation into connection.

---

## ğŸ§° Built With

**Languages & Frameworks**:  
HTML,CSS,JavaScript,VanillaJS,Mermaid.js

**Azure Services**:  
AzureTranslator,AzureSpeech-to-Text,AzureComputerVision,AzureOpenAI,gpt-35-turbo,AzureContentSafety

**Hosting & Dev Tools**:  
GitHubPages,GitHubCopilot,AzureStaticWebApps,AzureFunctions (optional)

---

## ğŸ› ï¸ How We Built It

- Frontend: HTML/CSS/JavaScript with Dark Mode UI and accessibility-friendly layout
- Integration with Azure Cognitive Services (Translator, Speech, Computer Vision)
- GPT-35-Turbo via Azure OpenAI for personalized suggestions
- GitHub Copilot assisted in smart coding and quick debugging
- Hosted via GitHub Pages and Azure Static Web Apps

---
## ğŸ” How It Works

1. **User Input**: The user can provide input via text, voice, or image upload.
2. **Processing**:
   - Voice input is converted to text using **Azure Speech-to-Text**
   - Images are analyzed and text extracted using **Azure Computer Vision (OCR)**
   - Raw text is passed to **Azure Translator** for language translation
3. **Tourist Spot Suggestion**: When a location is entered, **Azure OpenAI** generates relevant tourist recommendations.
4. **Output Filtering**: Optionally, **Azure Content Safety** ensures the output is appropriate and inclusive.
5. **Display**: Translations and suggestions are shown on the web UI with options to copy, switch modes, or interact further.

---

## ğŸ¥Demo


https://github.com/user-attachments/assets/73c10046-2ae2-4a93-babc-2190bb67a029


## ğŸ§  What We Learned

- How to orchestrate multimodal inputs in one interface
- Using Azure services responsibly with safety and performance
- Designing with empathy and language accessibility in mind
- Building a cohesive UX for tourists of all skill levels

---

## ğŸ What's Next

- ğŸ§­ Add geolocation-based tourist recommendations
- ğŸ“² Launch as a mobile PWA
- ğŸ”Š Integrate speech output (text-to-speech)
- ğŸ’¾ Add translation history and offline mode

---

## ğŸ¤ Contributing

Pull requests are welcome! For major changes, please open an issue first.

---

## ğŸ“„ License

MIT Â© 2024 LinguaLift Team
